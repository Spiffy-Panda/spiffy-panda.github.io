{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Panda's Portfolio","n":0.707},"1":{"v":"\n# [[Inverse Kinematics|personal_projects.IK]]\n#### Core Skills:\n[[tags.lang.C_sharp]][[tags.engine.unity]][[tags.robotics]][[tags.3d_mathematics]]\n\n![](/assets/MultiArmIK.png){width: 30%, opacity: 0.7, float: right, margin: 20px, border: 5px solid black}\nInstead of starting with a set of rotations and offsets to obtain an end position from base. Inverse Kinematics starts with an end position and set of constraints and calculates the rotations and offsets that solve them. \n\nMy approach uses unity's *ArticulatedBody* and it's Jacobian to preform a gradient decent. \n\n \n# [[personal_projects.ColorSpace]]\n#### Core Skills:\n[[tags.lang.Python]][[tags.lang.C_sharp]][[tags.toolkit.notebooks]][[tags.engine.unity]]\n\n![](/assets/CIE_LChab_StraightLine.jpg){width: 30%, opacity: 0.7, float: right, margin: 20px, border: 5px solid black}\nSimple Gradients are often made by *lerp* from one color to another. This is dependent on the color space, *sRGB* will produce different gradients then *HSV*. \n\nIn this project I try out different color spaces, *HSV*, *sRGB*, *XYZ*, *Lab* and *Lch*. \n\n# [[personal_projects.QOI_ImageFormat]]\n#### Core Skills:\n[[tags.tech_reading]][[tags.lang.C_sharp]][[tags.engine.unity]]\nPNG the one of the most widely used lossless image compression. In order to compress and decompress images, it uses a large number of different chunk types and calculates are a variety of metrics. As a result, PNG are slow and ram intensive to encode. QoI, is a proposed image format from Dominic Szablewski, that is meant to be quick and light instead. QoI's specification is also just 1 page, compared to PNG's 96 pages.\n\nI was interested in checking out this lossless image compression. In the passed I bit-banged out *BMP*s and studied *JPEG* in computation mathematics. I chose unity, thinking in the future it might be nice to use this in a larger project. \n\n### Older Projects\n\n# [[personal_projects.VR_Minimap]] and [[personal_projects.AR_Minimap]]\nFull 3D minimap using a second camera, changing the Interpupillary distance (IPD) to \"shrink\" the world. Followed up by a project to make the construction viewable in AR.\n\n# [[personal_projects.scrolling_circle_world]]\nExploring the workflow for a game that scrolls on a disk.","n":0.059}}},{"i":2,"$":{"0":{"v":"Tags","n":1}}},{"i":3,"$":{"0":{"v":"Toolkit","n":1}}},{"i":4,"$":{"0":{"v":"IPython Notebooks","n":0.707}}},{"i":5,"$":{"0":{"v":"Lang","n":1}}},{"i":6,"$":{"0":{"v":"Python","n":1}}},{"i":7,"$":{"0":{"v":"C#","n":1}}},{"i":8,"$":{"0":{"v":"Engine","n":1}}},{"i":9,"$":{"0":{"v":"Unity","n":1}}},{"i":10,"$":{"0":{"v":"Technical Reading","n":0.707}}},{"i":11,"$":{"0":{"v":"Robotics","n":1}}},{"i":12,"$":{"0":{"v":"3D Mathematics","n":0.707},"1":{"v":"\nThis is my strongest skill when compared with peers. \n\nI was a math major and computer science major. Between the two of curriculums, I learned the mathematic core behind 3D simulations. CS gave a very application driven view of 3D vector, Quaternions, Homogeneous Coordinates, Scene Graphs, and Transforms Matrices. Math came at it from the other direction, it gave a strong foundation of vectorization, General Transformations Matrices, Eigen Vectors/Values, Inverting Functions/Matrices, and Numerical methods. Numerical Methods are particularly useful when working with data that needs integrated or interpolated. ","n":0.107}}},{"i":13,"$":{"0":{"v":"Personal Projects","n":0.707}}},{"i":14,"$":{"0":{"v":"Scrolling Circle World","n":0.577},"1":{"v":"Language: Unity3D using Shaderlab/HLSL1\n\nInspiration: Indie Game \"Expand\" (Steam)\n\nGoal: Explore the workflow for a game that scrolls on a disk.\n\nApproach: Map the visuals of a side scroller to a disk via Shader.\n\nThe first half of the approach to get the visuals out put on the disk. Todo this, I first render the Scene as a typical sidescroller. Then, using Unity's Post Processing stack, I apply a shader to the whole screen.\n\nThe shader takes the given uv coordinates, centers them on zero and then translates them to polar coordinates. These new polar coordinatesare used to index into the texture resulting in the desired effect.\n\nThe game itself can just be a normal sidescroller, and needs little modification. Such modifications would be to change where the \"seam\" is and make mechanics aware of the screen space, such as adjust scale and controls.\n\n\n\nC# for loading and interaction.\n\nUnity's ShaderLab for distortion.","n":0.083}}},{"i":15,"$":{"0":{"v":"VR_Minimap","n":1},"1":{"v":"Language: Unity3D using C#\n\nGoal: Full 3D minimap\n\nApproach: Uses a second camera, changing the Interpupillary distance (IPD) to shrink the world.\n\nUnity3D's stereo camera automatically places itself at the location of the HMD. The IPD scaling and location scaling is done using Unity3D's built transform. Next, I changed the settings of the Minimap camera. Turned off the buffer clears so they render using the same Z-buffer. Then scaled the second Z-buffer by changing the near and far clipping plan to match the scale.\n\nThis works and shows up in world, while still being properly occluded. I then shift it based center of the map and location in world it.","n":0.097}}},{"i":16,"$":{"0":{"v":"Quite OK Image Format","n":0.5},"1":{"v":"\n\n#### Core Skills:\n[[tags.lang.C_sharp]][[tags.engine.unity]]\n\n## Abstract\nImplementation of the Quite OK Image Format (QoI)[^1] a lossless image compression algorithm in $O(n)$.\n\n## Goal\nStarting with just the specification[^2] (and an online decoder for validation), I wanted to implement the QoI format in C#, with unity as a visualizer. \n\n## Approach\nI started with the encoder, and took it in stages, names starting with *qoi_ * are from the spec[^2] but implemented by me.\n - implementation: minimal version\n    - *qoi_header* struct and a tailing 7 0x00 and one 0x01 bytes\n    - *qoi_op_rgb* and *qoi_op_rgba* the literals and base case for the \"chunks\" to come\n - I checked that with an online decoder to see that the image came out as expected. \n - implement the running array and *qoi_op_index* (see spec for details)\n - As the rest of the chuck encodings are based on differentials and \"runs\", I made a quick image gradient generator. That way I could control how fast the values change, causing the different encodings to be favored. \n    - *qoi_op_diff* for small changes\n    - *qoi_op_luma* for larger changes\n    - *qoi_op_run* for unbroken sequences of the previous pixel.\n\n\n## Results\n - Successfully implemented the standard in unity. It can both read and write files. \n## Next Steps\n - I would like to write some tests and visualizers as a demo site.\n\n\n[^2]: https://qoiformat.org/qoi-specification.pdf\n[^1]: https://qoiformat.org/","n":0.068}}},{"i":17,"$":{"0":{"v":"IK","n":1},"1":{"v":"\n# Inverse Kinematics or Unity's Articulated Bodies\n\n## Abstract\n- While designing a boss enemy for a game, I decided I wanted to try animating it with Inverse Kinematics (IK). So I set about preforming a feasibility test, something I am very familiar with from work. \n\n## Goal\n- Investigate Possible Middle Ware\n- Implement a classic multi-segment arm\n- Implement a IK to point to target\n- Extend it to handle different cases\n\n## Approach\n\n- Found out that Unity has a new physics component called *ArticulatedBody* that provides a dense *ArticulatedJacobian*. It is made for exactly this use case so bingo.\n- Math background:\n    - The Jacobian $J(\\theta)$ is a collection of all the \"first-order partial derivatives\"[^1] of a function. In the context of IK, \"it can provide relation between joint velocities and end effector velocities\"[^2]. So for a given set joint velocities $\\dot\\theta$ left hand multiplication provides the task space velocites $\\dot x$. This is only valid at the current state $\\theta$\n    - $$\\dot x=J(\\theta)\\dot\\theta$$\n    - We can use $J(\\theta)$'s pseudo inverse $J^+ = J^T J$ to take the error between the current and target location in task space $\\mathbf{e} = \\mathbf{x}_{target} - \\mathbf{x}_{current}$ to joint space. This is only valid for the current state $\\mathbf\\theta$ and since we are using the pseudo inverse $J^+$ as an approximation of $J^{-1}$, we only really get a direction to the goal with a magnitude can only be used as a hint. \n- A large portion of work on this project was digging into unity's *ArticulatedBody* and their...eccentricities. This is a area that is very recent and poorly documented. The example project didn't not preform Inverse Kinematics and just mapped user inputs to direct drive.\n- The actual solution was quite small I used *ArticulatedBody*'s *GetDenseJacobian*, *GetDofStartIndices* and *SetDriveTargetVelocities* only. The first to calculate the velocity to set the joints' drive and the other two send it to all the child components at once.\n- When I sat down to extend it to more joints, I found my solution just worked. Which is the advantage of this methodology opposed to one that uses more direct knowledge of the robots kinematics. \n\n## Results\n- The robot arm can track a target, mostly. While it can generally move towards a point. This approach is just simple gradient decent and has the problems of local minimum and vanishing gradient. It also is prone to oscillations were it keeps over shotting the objective.\n- Next steps would be to scale the velocity better. \n    - There are ways to calculate a scalar that minimizes the new error[^3].\n    - Also, the current solution behaves like a poorly tuned PD-controller, we could implement a tuned PID-controller.\n    - This method is ultimately just a gradient decent with one real-time state. If it is try out other optimizations forms, that would over come the local minimum. *GetDenseJacobian* is run of the current state of the unity physics engine, so I would need to find a way to have multiple hidden states.\n\n## Conclusion\n- The library is not production ready.\n- Using the Jacobian produces a good local optimizer\n- Finding a way to pre-plan multiple paths is need for a global optimizer.\n- Unity does not support this feature well despite *ArticulatedJoints* being in the core dlls and not a addon.  Meaning trusting unity's first party libraries when grading feasibility can lead to inaccurate time estimations.\n    - REQUIRES v2022.2.0b to work, which it doesn't say anywhere or use in the example project. With out it, at some point unity will give up and crash after each exit of play-in-editor.  \n\n[^1]: Wikipedia\n[^2]: Unity Scripting Reference\n[^3]: http://graphics.cs.cmu.edu/nsp/course/15-464/Spring11/handouts/iksurvey.pdf","n":0.041}}},{"i":18,"$":{"0":{"v":"Color Space","n":0.707},"1":{"v":"\n#### Core Skills:\n[[tags.lang.Python]][[tags.lang.C_sharp]][[tags.toolkit.notebooks]][[tags.engine.unity]]\n\n![Sample Gradient Short Paths](/assets/photo_2022-07-05_20-32-09.jpg){width: 200px}\n![Sample Gradient w/long paths](/assets/photo_2022-07-06_17-46-13.jpg){width: 215px}\n## Abstract\n- Inspired by a post showing the different between a sRGB lerp vs HSV lerp. I decided to explore the color space my self.\n- I then port some of the code to unity so that I could use it in the future if needed. \n\n## Goals\n- Explore ways to make gradients using different color spaces and paths. \n- Implement, in unity, CIE L\\*a\\*b\\* and HCL or LCh which i will refer to as just LCh for Luminance-Chroma-Hue. \n\n\n## Approach\n- As the objective is exploration a scripting language with many libraries makes a great environment. Specifically I used a Colab Notebook[^1] for repeatability. I found several Color tool boxes, each have their own way of doing things. \n- I implemented LCh From Lab along the way, however this may not have been necessarily as a form of it might be in the perception tool kit with another name, (Luv).\n- Implemented a \"color picker\" for quicker exploration. This color picker was in sRGB and two will act as the only user input as starting color and end color.\n- These would first be translated these points to each color space. A straight line between the new points, than translated back to sRGB for *matplotlib.pyplot.imshow* to display.\n- Added a \"reflex\", noted by an *r* suffix in the plots, version of the cylinder color space, this rotates the hue by $180^\\circ$ so the path could go through the red hues. \n- After confirming that LCh does produce nice looking gradients, I ported the color transformations to unity. In unity I made a grid of points in sRGB, and a mesh \"ribbon\" between two points that is a straight line in LCh. \n- For one last visualizations, I add a slider that could change the currently rendered color space. So instead of looking at sRGB your could see the straight path in LCh, will blending along the transformation chain: $sRGB \\rightarrow XYZ\\rightarrow CIE\\ \\ La^*b^*\\rightarrow LCh.$\n\n## Conclusion\n- The gradients came out much better when using a cylinder based color space, especially when that color space is based on *Lab*\n- It would be worth looking into finding a efficiently written\n- Might be worth trying to spin off [[Quite okay Image Format |personal_projects.Quite Ok Image Format]] with a Lch encoding.\n\n\n[^1]: https://colab.research.google.com/drive/1oB74E0J_cruK6b9RIyjHAGj38l4cB5RV?usp=sharing","n":0.051}}},{"i":19,"$":{"0":{"v":"AR_Minimap","n":1},"1":{"v":"\n\nLanguage: Unity3D using C#\n\nGoal: Allow second user to see the minimap using a AR phone.\n\nApproach: Used an augment reality SDK and tag on a table, then project the mini map content on top.\n\nWhile one player uses the HTC vive tracked controllers to build a fort, as in the VR mini map, another user can view the fort. This is achieved with two separate programs, the VR fort demo and a phone viewer. For development I used a webcam attached to my computer for the phone. The \"phone\" viewer is also written in Unity3d with the help of the ARToolkit. This worked for under Idea circumstances but ultimately was not meant for Unity.\n\n\n\nI switch over to Vuforia. This is SDK I saw an a /r/Unity3D and has much better Unity support. It is a Natural Feature Tracking solution. That does the preprocessing phase on the web, giving you a .unitypackage with the tracking info. For the tag, I lack a printer, so I made my own with some paint and heavy stock I had laying around.\n\n\n\nWhen the \"phone\" boots up, it sends connects to the server over .Net.Sockets. The server logs this client connection and sends any already placed items. The \"phone\" renders this place items based of the location of the Tag. The result is a way for a \"little brother\" to watch the player build.\n\n\n\nThe Network communication right now is hardcoded IPs. This is good in a test environment but should be switched to another solution. Possibly the VR host displaying a QR code with the host IP, being that this is only meant for LAN.\n\n\n\nWhile developing my communication format, I created a 2 WPF application for debugging. The first acted as a \"fake\" server, it send all the same massages but control the inputs by sliders. The second acted as a \"fake\" client, it posted all the incoming messages to a textbox, allowing a clear view of what is received. This server code was then shared with Unity3D for use with the VR setup. This allowed my to mix and match what server and client(s) I used.\n\n\n\nThis two WPF used NewtonSoft JSON.net for my serialization, which is not supported by Unity3D because of a .net version miss-match. For the Unity3D end, I used the lightweight JsonUtility built in to Unity3D. This utility only allowed for structed Json, so I made a new sturct that only had message type and payload as variables. This meant that I could send the incoming message off to the right parser in Unity.","n":0.049}}}]}
